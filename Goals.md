# âœ¨ NeuraRust Goals & Vision ğŸ¦€ğŸ§ 

**NeuraRust** aims to become a leading **Deep Learning framework in Rust**, combining the flexibility and ergonomics of PyTorch with the **raw performance**, **memory safety**, and **portability** offered by Rust.

---

## ğŸ¯ Our Core Pillars

*   ğŸš€ **Exceptional Performance:**
    *   Compete with C++/Python giants in execution speed (CPU & GPU).
    *   Minimize memory footprint thanks to Rust's precise control (no GC!).
*   ğŸ¤ **Intuitive Ergonomics:**
    *   A familiar and enjoyable API, inspired by best practices (PyTorch/Keras).
    *   Comprehensive documentation and accessible tutorials for a quick start.
*   ğŸ”„ **Seamless Interoperability:**
    *   Compatibility via **ONNX** to exchange models with PyTorch/TensorFlow.
    *   Smooth integration with the Python ecosystem using **PyO3**.
*   ğŸ”’ **Safety & Easy Deployment:**
    *   The Rust promise: **No segfaults, no unexpected memory leaks**.
    *   Native support for easy deployment to various targets: **WebAssembly (WASM)**, **ARM** (embedded/mobile), servers...

---

## ğŸ› ï¸ Core Features (PyTorch Inspired, Rust Superpowered)

We replicate PyTorch's essential building blocks, enhancing them with Rust:

### 1. Multi-Dimensional Tensors (`neurarust-core::Tensor`) ğŸ“

*   **Vision:** The framework's beating heart. Fast, safe, flexible.
*   **Key Points:**
    *   **Explicit and performant** memory management.
    *   Fine-grained control over memory layout (strides...).
    *   **Strong typing** to catch dimension/type errors at compile time.
    *   Mathematical, logical operations, indexing, **broadcasting** (implemented for Add)... everything needed!
*   **The Rust Advantage:** ğŸ’ª Guaranteed memory safety, native C/C++ performance, SIMD potential.

### 2. Automatic Differentiation (`neurarust-core::Autograd`) ğŸ“ˆ

*   **Vision:** A dynamic, reliable, and efficient autograd engine.
*   **Key Points:**
    *   **On-the-fly computation graph** construction.
    *   Simplified gradient calculation via **`.backward()`**.
    *   Optimized memory management for intermediate tensors.
    *   Backward pass implemented for core ops (e.g., Add with broadcasting).
*   **The Rust Advantage:** ğŸ§  The borrow checker tames graph complexity, "fearless" concurrency potential for accelerating computations.

### 3. Neural Network Modules (`neurarust-nn`) ğŸ§© *(Future)*

*   **Vision:** A comprehensive toolbox for assembling your networks.
*   **Key Points:**
    *   Standard layers: **Linear, Convolutional, Recurrent, Attention, Normalization...**
    *   Common activation and loss functions.
    *   **Composable and extensible** API for creating custom architectures.
*   **The Rust Advantage:** âœ¨ Traits for clear interfaces (`Module`, `Layer`), macros for less boilerplate.

### 4. Optimizers (`neurarust-optim`) âš™ï¸ *(Partially Implemented)*

*   **Vision:** Essential algorithms for training your models.
*   **Key Points:**
    *   Classics: **SGD (implemented)**, Adam, AdamW, RMSprop...
    *   Simple `Optimizer` interface to apply updates.
    *   Internal state management (e.g., moments).
*   **The Rust Advantage:** âš¡ Native performance, generic implementations via traits.

### 5. Data Loading (`neurarust-data`) ğŸ’¾ *(Future)*

*   **Vision:** Performant tools to feed your models.
*   **Key Points:**
    *   `Dataset` and `DataLoader` abstractions.
    *   **Batching, shuffling, performant parallel loading**.
    *   Utilities for transformations and augmentations.
*   **The Rust Advantage:** ğŸï¸ Robust parallelism ideal for I/O and preprocessing, efficient memory management.

### 6. Accelerator Support (GPU & Beyond) ğŸ”¥ *(Future)*

*   **Vision:** Unleash the massive computational power of dedicated hardware.
*   **Key Points:**
    *   **CUDA** integration (priority), then potentially ROCm, Metal, **WebGPU**.
    *   `Device` abstraction (CPU, GPU:0...).
    *   Transparent CPU <-> GPU data transfer.
*   **The Rust Advantage:** ğŸŒ Existing bindings, safe abstractions, WebGPU (written in Rust) as a portable target for the future.

### 7. Interoperability & Deployment (`neurarust-deploy`) ğŸŒ *(Future)*

*   **Vision:** Integrate everywhere, deploy easily.
*   **Key Points:**
    *   **ONNX** for model exchange.
    *   **PyO3** for symbiosis with Python.
    *   **WASM** compilation for web and serverless.
    *   Easy cross-compilation (e.g., **ARM**).
    *   **Native, standalone, and performant** binaries.
*   **The Rust Advantage:** ğŸ“¦ First-class WASM/ARM support, mature FFI, easy static binary distribution.

---

## ğŸ’ Our Differentiators: The Unique Rust Advantage

Beyond PyTorch parity, we aim to fully leverage Rust to offer:

*   **First-Class WASM Support ğŸ•¸ï¸:** Performant and lightweight inference in the browser and on the edge. Revolutionizing interactive and embedded ML.
*   **Enhanced Safety Guarantees âœ…:** Go further in verification and robustness using the type system for critical applications.
*   **Advanced Static Optimizations ğŸš€:** Use macros to optimize graphs *at compile time* (op fusion, etc.) for more performance with no runtime overhead.
*   **Simplified & Safe Parallelism â›“ï¸:** High-level APIs to leverage multi-core and distributed computing without fearing data races.

---

## ğŸ—ºï¸ Highly Detailed Roadmap (PyTorch Parity Goal)

This roadmap outlines the planned development stages for NeuraRust, aiming for extensive feature parity with PyTorch over time. Status markers: âœ… (Done), ğŸš§ (In Progress / Partially Done), â³ (To Do).

**Phase 0: Foundations & Core Tensor [âœ… Done]**
*   ğŸ¯ **Goal:** Establish project structure, implement basic CPU `Tensor` with core functionalities.
*   **0.1 Project Setup [âœ… Done]**
    *   âœ… Workspace Setup: Defined workspace in root `Cargo.toml`, configured basic CI, added `rustfmt.toml` and standard `clippy` lints.
    *   âœ… Licensing: Added `LICENSE` file (MIT/Apache 2.0 chosen).
    *   âœ… Contribution Docs: Created `CONTRIBUTING.md` and `CODE_OF_CONDUCT.md`.
*   **0.2 Core `Tensor` Struct (`neurarust-core`) [âœ… Done]**
    *   âœ… `Tensor` Struct Definition (`tensor::mod.rs`): Created the main user-facing `Tensor` struct.
    *   âœ… `TensorData` Struct Definition (`tensor_data.rs`): Internal struct holding core data.
    *   âœ… Data Storage: Implemented using `Rc<RefCell<Vec<T>>>` within `TensorData`.
        *   ğŸ“ *Note:* This choice enables basic sharing needed for dynamic autograd graph construction (multiple `Tensor`s can refer to the same `TensorData`). However, `RefCell` enforces runtime borrow checking and is *not thread-safe*, which **limits future parallelism** (e.g., using `rayon` for parallel CPU ops or multi-threaded data loading accessing tensors). Evolution towards thread-safe structures like `Arc<Mutex/RwLock>` or potentially specialized concurrent data structures will be necessary in later phases (especially Phase 4/6).
    *   âœ… Shape Representation: Implemented using `shape: Vec<usize>` field in `TensorData`.
    *   âœ… **Strides Representation:** **Implemented!** Added `strides: Vec<usize>` field to `TensorData`. `Tensor::new` now calculates contiguous strides by default. This resolves the **critical prerequisite** for views (Phase 1.4).
    *   âœ… Basic Creation Methods:
        *   âœ… `Tensor::new(data: Vec<T>, shape: Vec<usize>)`: Acts like `from_vec`, consuming data, calculates strides.
        *   âœ… `Tensor::zeros_like(&self)`: Creates a tensor of zeros with the same shape.
        *   âœ… **Standalone Creation Functions:** Implemented `neurarust::tensor::zeros(shape)`, `neurarust::tensor::ones(shape)`, `neurarust::tensor::full(shape, value)` for better ergonomics.
    *   âœ… Initial Data Type Support: Generic `<T>` used, primarily focused on `f32` via trait bounds like `Copy`, `Debug`, `PartialOrd`, `Add`, `Sub`, `Mul`, `Div`, `Neg`. Explicit `DType` enum and multi-type support are missing (Phase 1.4).
*   **0.3 Basic CPU Operations (`neurarust-core::ops` - Forward Pass Only) [âœ… Done]**
    *   âœ… Element-wise Arithmetic (`ops::arithmetic`): Forward pass implemented for `add`, `sub`, `mul`, `div`, `neg`. These handle basic tensor-tensor and tensor-scalar operations.
    *   âœ… Broadcasting Utilities (`tensor::utils`): Implemented `broadcast_shapes` helper and logic to determine compatible shapes for broadcasting.
    *   âœ… Add Operation with Broadcasting: Forward pass specifically handles broadcasting.
    *   âœ… **Stride-Aware Indexing:** Added `TensorData::get_offset` method. Forward passes for `add`, `sub`, `mul`, `div`, `neg` correctly use `get_offset` for data access, making them compatible with strides. (Note: `matmul` removed in cleanup).
    *   â³ **Basic Backward Infrastructure:** Definition of `BackwardOp` trait removed during cleanup. Reintroduction needed for Phase 1.
*   **0.4 Initial Testing [âœ… Done]**
    *   âœ… Basic Unit Tests: Added tests covering `Tensor` creation, shape validation, basic arithmetic operations (forward pass), broadcasting utility functions, and new creation functions.
*   **0.5 Overall Status & Key Issues [âœ… Done]**
    *   **Status:** Project structure and foundational `Tensor` struct are implemented with explicit stride support. Basic element-wise operations (`add`, `sub`, `mul`, `div`, `neg`) use strides for data access on CPU. Standalone creation functions added. Core error handling implemented. Codebase cleaned of Phase 1-3 elements, tests pass.
    *   âœ… **Critical Issue (Lack of strides): Resolved.** `TensorData` now stores strides, and basic operations use them for indexing.
    *   âœ… **Strides Stored for Views:** Strides are stored in `TensorData`, providing the prerequisite for views. â³ **View Implementation (Phase 1):** Operations like `reshape`, `slice`, `transpose` need to be implemented/re-implemented as true views (sharing data) in Phase 1.
    *   âœ… **Error Handling Improvement:** Addressed. Core functions like `Tensor::new`, `sum_axes` return `Result<T, NeuraRustError>`, handling common errors like shape mismatches or invalid indices gracefully.
    *   âœ… **Thread-Safety for Parallelism:** Replaced `Rc<RefCell<TensorData<T>>>` with `Arc<RwLock<TensorData<T>>>`. Internal data buffer uses `Arc<Buffer<T>>`. This provides the necessary thread-safety foundation for future parallel computation (e.g., CPU via Rayon, GPU acceleration - Phases 4/6), resolving the limitation noted previously.

**Phase 1: Views, Autograd & Expanded CPU Ops [â³ To Do]**
*   ğŸ¯ **Goal:** Implement view semantics, a functional dynamic autograd system, and significantly expand CPU tensor operations & API, **ensuring compatibility with the new `Arc<RwLock>`, `Buffer`, and `StorageDevice` structures.**

*   **1.1 View Semantics & Core Shape Ops [â³ To Do]**
    *   ğŸ¯ Goal: Implement non-copying views for shape manipulation, essential for performance and memory.
    *   â³ **Refine `TensorData::new_view`:** Ensure it's accessible (e.g., `pub(crate)`) and correctly takes `Arc<Buffer<T>>`, `device`, `offset`, `shape`, `strides` to create `TensorData` instances representing views.
    *   â³ **Implement `slice` Operation:**
        *   Define `slice_op(tensor: &Tensor<T>, /* slice args */) -> Result<Tensor<T>>`.
        *   Inside, acquire read lock on input `tensor.data`.
        *   Validate slice arguments against `shape`.
        *   Calculate new `shape` and new `offset` based on original offset, slice args, and `strides`.
        *   Create new `TensorData` using `new_view` with cloned `Arc<Buffer<T>>`, original `device`, new `offset`, new `shape`, and *original* `strides`.
        *   Wrap in `Tensor { data: Arc::new(RwLock::new(new_td)) }`.
        *   Implement user-facing `Tensor::slice(...)` method.
    *   â³ **Implement `transpose` Operation:**
        *   Define `transpose_op(tensor: &Tensor<T>, dim1: usize, dim2: usize) -> Result<Tensor<T>>`.
        *   Acquire read lock, validate `dim1`, `dim2` against rank.
        *   Calculate new `shape` (swap dims) and new `strides` (swap strides).
        *   Create view using `new_view` (cloned buffer, original device/offset, new shape/strides).
        *   Implement `Tensor::transpose(...)`.
    *   â³ **Implement `permute` Operation:**
        *   Define `permute_op(tensor: &Tensor<T>, dims: &[usize]) -> Result<Tensor<T>>`.
        *   Acquire read lock, validate `dims` is a valid permutation for the rank.
        *   Calculate new `shape` and new `strides` by reordering according to `dims`.
        *   Create view using `new_view` (cloned buffer, original device/offset, new shape/strides).
        *   Implement `Tensor::permute(...)`.
    *   â³ **Implement `reshape` / `view` Operation:**
        *   Define `reshape_op(tensor: &Tensor<T>, new_shape: Vec<usize>) -> Result<Tensor<T>>`.
        *   Acquire read lock, validate `new_shape` product matches old product.
        *   Call `is_contiguous()` on the tensor.
        *   If contiguous: Calculate new *contiguous* `strides` for `new_shape`. Create view using `new_view` (cloned buffer, original device/offset, `new_shape`, new strides).
        *   If non-contiguous: Check if a view is *still possible* (i.e., if specific stride manipulation can achieve the reshape). If yes, calculate those strides and create view. If not possible as a view, return `Err`. (User must call `.contiguous().reshape(...)` explicitly).
        *   Implement `Tensor::reshape(...)` and potentially `Tensor::view(...)` (alias or stricter view-only version).
    *   â³ **Implement `contiguous()` Method:**
        *   Implement `Tensor::contiguous(&self) -> Result<Tensor<T>>`.
        *   Call `is_contiguous()`. If true, return `self.clone()`.
        *   If false:
            *   Acquire read lock.
            *   Get buffer reference (`cpu_data()?` for now). Get `device`, `shape`, `strides`, `offset`.
            *   Allocate a *new*, *contiguous* buffer (`Vec<T>` for now) on the **same `device`**.
            *   Iterate multidimensionally over `shape`.
            *   For each index set, calculate offset in the *original* buffer using `guard.get_offset()`.
            *   Read value from original buffer (CPU read for now).
            *   Write value to the *new* buffer at the current linear index.
            *   Create and return a *new* `Tensor` using `Tensor::new()` with the new buffer and shape (which calculates contiguous strides).
    *   â³ **Implement `is_contiguous()` Check:**
        *   Implement `TensorData::is_contiguous(&self) -> bool`.
        *   Calculate expected contiguous strides for `self.shape`.
        *   Compare `self.strides` with expected strides (handle 0/1 dim sizes).
        *   Implement `Tensor::is_contiguous(&self)` calling the `TensorData` method via read lock.
    *   â³ **Testing:**
        *   Unit tests for each view op (`slice`, `transpose`, `permute`, `reshape`/`view`).
        *   Test edge cases (empty tensors, 0/1 sized dimensions).
        *   Verify views share the underlying buffer (`Arc::ptr_eq` on `borrow_data_buffer()`).
        *   Verify correct `shape`, `strides`, `offset`, `device` for views.
        *   Test `is_contiguous()` correctly identifies contiguous/non-contiguous tensors.
        *   Test `contiguous()` copies only when necessary and produces a contiguous tensor on the correct device.
        *   Test that data modifications via one view are reflected when accessing via another view (requires working `get`/`set` or similar).

*   **1.2 Basic Autograd Infrastructure [â³ To Do]**
    *   ğŸ¯ Goal: Re-establish the foundational components for automatic differentiation, **handling thread-safety and device awareness.**
    *   â³ **Add `TensorData` Fields:**
        *   Add `requires_grad: bool` (default `false`).
        *   Add `grad: Option<Tensor<T>>` (holds the gradient tensor, must be on same device).
        *   Add `grad_fn: Option<Arc<dyn BackwardOp<T> + Send + Sync>>` (using `Arc` for shared ownership of backward node, requires trait bounds).
    *   â³ **Define `BackwardOp<T>` Trait:**
        *   `pub trait BackwardOp<T: 'static + ...>: Debug + Send + Sync { ... }` (add relevant bounds for `T`).
        *   `fn backward(&self, grad_output: &Tensor<T>) -> Result<Vec<Tensor<T>>, NeuraRustError>;` (Must handle device consistency).
        *   `fn inputs(&self) -> Vec<*const RwLock<TensorData<T>>>;` (Returns stable IDs of input tensors).
    *   â³ **Implement `Tensor` Autograd Accessors/Mutators:**
        *   `fn requires_grad(&self) -> bool;` (read lock).
        *   `fn set_requires_grad(&self, requires_grad: bool) -> Result<(), NeuraRustError>;` (write lock, handle potential graph modifications).
        *   `fn grad(&self) -> Option<Tensor<T>>;` (read lock, clones `Tensor` if `Some`).
        *   `fn acc_grad(&self, grad_to_add: Tensor<T>) -> Result<(), NeuraRustError>;` (write lock, handles `None`, checks device, performs accumulation via device-aware `add_op`).
        *   `fn grad_fn(&self) -> Option<Arc<dyn BackwardOp<T> + Send + Sync>>;` (read lock, clones `Arc`).
        *   `fn set_grad_fn(&self, grad_fn: Option<Arc<dyn BackwardOp<T> + Send + Sync>>) -> Result<(), NeuraRustError>;` (write lock).
    *   â³ **Implement Graph Traversal (`autograd::graph`):**
        *   Implement topological sort function (e.g., Kahn's or DFS based).
        *   Takes starting `Tensor` pointer/ID.
        *   Uses `*const RwLock<TensorData<T>>` as node identifier.
        *   Traverses graph via `grad_fn` and `inputs()`. Needs read locks.
        *   Handles cycles (returns `Err`).
        *   Returns ordered list of node IDs for backward pass.
    *   â³ **Implement `Tensor::backward()` Logic:**
        *   `fn backward(&self, gradient: Option<Tensor<T>>) -> Result<(), NeuraRustError>;`
        *   Check `self.requires_grad()`.
        *   Determine initial gradient (use provided `gradient`, or default to `1.0` scalar if `self` is scalar, error otherwise). Ensure initial grad is on `self.device()`.
        *   Perform topological sort from `self`.
        *   Prepare gradient accumulation map: `HashMap<NodeId, Tensor<T>>`, initialized with initial gradient for `self`.
        *   Iterate through sorted nodes:
            *   Retrieve accumulated grad for current node from map.
            *   Acquire read lock for current node `TensorData` to get `grad_fn`.
            *   If `grad_fn` is `Some(op)`:
                *   Call `op.backward(&accumulated_grad)` -> returns input grads.
                *   Get input node IDs from `op.inputs()`.
                *   For each input node ID and its calculated grad:
                    *   Retrieve the input `Tensor` corresponding to the ID (needs mechanism to map ID back or pass `Tensor`s).
                    *   If input `tensor.requires_grad()`:
                        *   Call `input_tensor.acc_grad(calculated_grad)` (handles locking, device checks, accumulation).
            *   Optionally clear `grad` field for non-leaf nodes after processing (memory optimization).
            *   Optionally clear `grad_fn` if `retain_graph=false` (default `false`).

*   **1.3 Core Op Implementation (Forward & Backward) [â³ To Do]**
    *   ğŸ¯ Goal: Implement forward and backward passes for essential operations, **ensuring correct handling of locks, views/strides, buffers, and devices.**
    *   â³ **General Pattern:**
        *   Forward (`*_op`): Takes `&Tensor` inputs. Acquires read locks. Performs device checks. Accesses data via `buffer.cpu_data()?` (for now). Creates result `Tensor` on correct device. If autograd needed, create `*Backward` struct, wrap in `Arc`, set `grad_fn` on result.
        *   Backward (`*Backward` struct): Implements `BackwardOp`. Stores necessary context (input shapes, IDs, maybe data). `backward` method calculates input gradients using `grad_output` and context, respecting devices.
    *   â³ **Implement `div`, `neg`, `pow` (Forward & Backward):** Follow general pattern. Handle chain rule, device awareness. `DivBackward` needs care with division by zero.
    *   â³ **Implement/Adapt `reduce_gradient` Utility:** Takes gradient and target shape. Performs device-aware sum reduction along broadcasted axes.
    *   â³ **Implement View Backwards (`SliceBackward`, `TransposeBackward`, `ReshapeBackward`, `PermuteBackward`):**
        *   Implement backward logic (often involves scattering/indexing gradients). Must be device-aware.
    *   â³ **Implement `matmul` (Forward & Backward - 2D):** Implement CPU forward (loop or BLAS placeholder). Implement `MatMulBackward` (matrix math, device-aware).
    *   â³ **Implement `sum_axes`, `mean` (Forward & Backward):** Implement CPU forward. Implement `SumAxesBackward`, `MeanBackward` (often broadcasting grad_output, device-aware).
    *   â³ **Implement `relu` (Forward & Backward):** Implement CPU forward. Implement `ReluBackward` (conditional gradient propagation, device-aware).

*   **1.4 Tensor API & Data Type Expansion [â³ To Do]**
    *   ğŸ¯ Goal: Enhance `Tensor` usability, add multi-type support, **considering device management.**
    *   â³ **Implement Creation Methods (`arange`, `linspace`, `eye`, `rand`, `randn`):** Functions take shape/range and optional `dtype: Option<DType>`, `device: Option<StorageDevice>` (defaults CPU). Dispatch based on `T` and generate data on correct device.
    *   â³ **`DType` Handling:** Decide approach: Keep `Tensor<T>` generic, use traits/macros for op dispatch based on `T`. Avoid `AnyBuffer` initially.
    *   â³ **Type Promotion:** Implement logic within ops or helper functions to cast inputs before operation based on promotion rules (e.g., `i32+f32 -> f32`).
    *   â³ **Implement Type Conversion:** `Tensor::cast<NewType>(&self) -> Result<Tensor<NewType>>`. Iterate data on original device, cast, create new Tensor on same device.
    *   â³ **Implement `detach()`:** `Tensor::detach(&self) -> Tensor<T>`. Clones `Arc<RwLock>`, gets write lock on new data, clears `requires_grad`, `grad_fn`, `grad`. Returns new `Tensor`.
    *   â³ **Implement In-place Ops (`add_`, `mul_`, ...):**
        *   `Tensor::add_(&self, other: &Tensor<T>) -> Result<(), NeuraRustError>;`
        *   Requires `&self` (mutable access is via interior mutability).
        *   Acquire `write` lock on `self.data`.
        *   Perform device checks (`self` vs `other`).
        *   Check safety: Return `Err` if `self.requires_grad()` (simplest rule initially).
        *   Perform operation directly on `self.data.data` buffer (CPU/GPU specific logic).

*   **1.5 Testing (Numerical Gradients) & Documentation [â³ To Do]**
    *   ğŸ¯ Goal: Rigorously verify gradient calculations and document Phase 1 features, **including new structures and device concepts.**
    *   â³ **Implement Numerical Gradient Checking Utility:**
        *   Function takes `Fn(&Tensor<T>) -> Result<Tensor<T>>`, input `Tensor<T>`, `epsilon`.
        *   Needs to handle device (e.g., move inputs to CPU for calculation, run function, move result back if needed, or run entirely on one device if possible).
        *   Calculates finite difference approximation.
        *   Compares with analytical gradient obtained via `input.grad()` after `output.backward()`.
        *   Handle precision issues.
    *   â³ **Add Gradient Check Tests:** Create tests using the utility for *every* implemented `BackwardOp` on CPU.
    *   â³ **Expand Unit Tests:**
        *   Test all view ops, API methods (`cast`, `detach`, creation ops), forward/backward ops.
        *   Include tests for device arguments and checks.
        *   Test autograd graph construction, traversal, `backward` execution.
        *   Test error conditions extensively.
    *   â³ **Consider Property-Based Testing (`proptest`):** Define strategies for `Tensor` generation (shapes, CPU device) and test fundamental properties.
    *   â³ **Documentation (`rustdoc`, Guides):**
        *   Explain `Arc<RwLock>` pattern for `TensorData` and thread-safety implications.
        *   Explain `Buffer` abstraction and `StorageDevice` (initially CPU focus).
        *   Document view semantics (sharing, strides, offset, contiguity).
        *   Document basic autograd usage (`requires_grad`, `backward`, `grad`).
        *   Document all new public APIs with examples.
        *   Create initial user guide sections on core concepts.

*   **Overall Status Phase 1:** Ready to start. Focus is on implementing views first, then re-building autograd and ops **with thread-safety and device awareness (CPU initially) from the ground up.**

**Phase 2: Neural Network Primitives & Optimization [â³ To Do]**
*   ğŸ¯ **Goal:** Build foundational `nn` modules, loss functions, and optimization algorithms to enable basic model definition and training, **integrating device management (`CPU`/`GPU`) and leveraging the thread-safe `Tensor` structure.**
*   **2.1 NN Module System (`neurarust-core::nn`) [âŒ Not Implemented]**
    *   ğŸ¯ Goal: Define the core abstractions for building neural networks, **aware of device placement.**
    *   âŒ **`Module` Trait:** **Missing.** Needs methods like `.to(device)`, `.device()`, `.parameters()`, `.buffers()`, `train()`, `eval()`. Must handle recursive application to submodules.
    *   âŒ **`Parameter` Struct:** **Missing.** Needs to wrap a `Tensor` configured with `requires_grad=true`. The `Tensor` internally handles `Arc<RwLock>`, `Buffer`, and `device`.
    *   âŒ **Module Containers:** **Missing.** (`Sequential`, `ModuleList`, `ModuleDict`). Need to correctly manage submodules, parameters, and device transfers (`.to(device)`).
    *   âŒ **Helper Methods:** **Missing.** (`named_parameters`, `train`, `eval`, etc.).
*   **2.2 Core Layers (`neurarust-core::nn::layers`) [âŒ Not Implemented]**
    *   ğŸ¯ Goal: Implement fundamental neural network layers, **handling device placement and device-aware operations.**
    *   âŒ **Linear Layer:** **Missing.** Constructor needs `device` argument. `forward` must ensure input and weights are on the same device and call device-aware `matmul`.
    *   âŒ **Missing Layers:** All standard layers missing (Conv, Pool, Norm, RNN, etc.). All require device-aware initialization and `forward` implementations using device-aware backend ops.
*   **2.3 Loss Functions (`neurarust-core::nn::losses`) [âŒ Not Implemented]**
    *   ğŸ¯ Goal: Implement standard functions for calculating training loss, **operating on tensors located on a specific device.**
    *   âŒ **Mean Squared Error:** **Missing.** Must check input/target device consistency and perform calculation on that device.
    *   âŒ **Missing Loss Functions:** All standard losses missing (CrossEntropy, BCE, etc.). Require device checks and device-aware computation.
*   **2.4 Weight Initialization (`neurarust-core::nn::init`) [âŒ Not Implemented]**
    *   ğŸ¯ Goal: Provide standard techniques for initializing layer weights **directly on the target device.**
    *   âŒ Module `nn::init` **does not exist**.
    *   âŒ All initializers missing. Need to operate on the `Tensor`'s `Buffer` according to its `device` (potentially requiring data generation on CPU then transfer, or direct GPU random generation - Phase 4).
*   **2.5 Optimizers (`neurarust-optim`) [âŒ Not Implemented]**
    *   ğŸ¯ Goal: Implement algorithms for updating model weights based on gradients, **handling parameters and optimizer state potentially residing on different devices.**
    *   âŒ **Crate `neurarust-optim` removed.** Decision needed: new crate or integrate into `neurarust-core`.
    *   âŒ **`Optimizer` Trait:** **Missing.** `step()` method needs to handle parameters/gradients on potentially different devices.
    *   âŒ **SGD Implementation:** **Missing.** Weight updates must occur on the parameter's device.
    *   âŒ **Adam Implementation:** **Missing.** Requires device-aware updates and storing optimizer state (moments) as `Tensor`s on the same device as the parameters.
    *   âŒ All other optimizers missing.
*   **2.6 Learning Rate Schedulers (`neurarust-optim::lr_scheduler`) [âŒ Not Implemented]**
    *   ğŸ¯ Goal: Provide methods for adjusting the learning rate during training.
    *   âŒ Module `lr_scheduler` **does not exist**.
    *   âŒ All schedulers missing. (Less directly impacted by device, but interface with device-aware `Optimizer`).
*   **2.7 Integration & Training Loop [âŒ Not Implemented]**
    *   ğŸ¯ Goal: Demonstrate how the components work together, **including explicit device management.**
    *   âŒ Test file removed. No example exists. Needs to show `model.to(device)`, `data.to(device)`, loss calculation, backward pass, and optimizer step all respecting the chosen device.
*   **2.8 Serialization [âŒ Not Implemented]**
    *   ğŸ¯ Goal: Enable saving and loading model and optimizer states, **preserving device information or allowing device remapping.**
    *   âŒ No saving/loading capabilities exist. Needs to handle `device` metadata for parameters/buffers/optimizer state. `load_state_dict` needs a `map_location` argument.
*   **2.9 Testing & Documentation [âŒ Not Implemented]**
    *   ğŸ¯ Goal: Ensure correctness of NN components and provide clear documentation, **covering device management extensively.**
    *   âŒ **Unit Tests:** Missing. Need tests covering different device scenarios (CPU, GPU when available).
    *   âŒ **Integration Tests:** Missing. Needs training loop tests on different devices.
    *   âŒ **Documentation:** Missing. Needs detailed explanation of device handling (`.to()`, parameter initialization, optimizer state, training loops).
*   **Overall Status Phase 2:** **Not started.** All components related to this phase were removed. Requires Phase 1 completion. **Implementation must be device-aware from the start.**

**Phase 3: Data Loading & Handling (`neurarust-data`) [â³ To Do]**
*   ğŸ¯ **Goal:** Develop robust and performant tools for data loading, preprocessing, and augmentation, **ensuring efficient batch creation on the target device and leveraging thread-safe structures.**
*   **3.1 Dataset Abstractions [âŒ Not Implemented]**
    *   ğŸ¯ Goal: Define standard interfaces for accessing datasets.
    *   âŒ **Crate `neurarust-data` removed.** Decision needed: new crate or integrate.
    *   âŒ **`Dataset` Trait:** **Missing.** (Less impacted by device directly).
    *   âŒ **`VecDataset`:** **Missing.** (If returns Tensors, needs default device).
    *   âŒ **`IterableDataset` Trait/Concept:** **Missing.** (Less impacted by device directly).
*   **3.2 DataLoader [âŒ Not Implemented]**
    *   ğŸ¯ Goal: Provide an iterator for efficient batching, shuffling, and loading of datasets, **with device-aware collation and GPU transfer optimizations.**
    *   âŒ **`DataLoader` Struct:** **Missing.**
    *   âŒ **Missing Core Features:**
        *   Batching: Needs implementation.
        *   Shuffling: Needs implementation.
        *   **Custom Collation:** Needs `collate_fn` argument. Default `collate_fn` must create batch `Tensor`s **on a specified device (configurable, default CPU)**.
        *   **Parallel Loading:** Needs `num_workers` > 0 support. Collation must be thread-safe and place result on target device.
        *   Samplers: Missing.
        *   **Memory Pinning:** Needs `pin_memory` option. If true, collation for CPU tensors should use pinned memory (requires Phase 4 backend integration, e.g., `cudaMallocHost`).
        *   Worker Init: Missing.
        *   Persistent Workers: Missing.
        *   **Automatic Device Placement:** Consider adding option to move batch to target device automatically.
*   **3.3 Data Preprocessing & Augmentation (`neurarust-vision`, `neurarust-text`?) [âŒ Not Implemented]**
    *   ğŸ¯ Goal: Provide tools for transforming and augmenting data samples.
    *   âŒ **No Transform Module:** Missing.
    *   âŒ All transforms (Vision, Text, Generic) missing. (Transforms outputting Tensors need default device. Transforms using Tensor ops need device awareness).
*   **3.4 Integration & Utilities [âŒ Not Implemented]**
    *   ğŸ¯ Goal: Provide helpers for common dataset tasks and formats.
    *   âŒ **No Dataset Utilities:** Missing.
    *   âŒ Common Dataset Helpers missing.
    *   âŒ Splitting Datasets missing. (Utilities handling Tensors need device awareness).
*   **3.5 Testing & Documentation [âŒ Not Implemented]**
    *   ğŸ¯ Goal: Ensure correctness and provide clear documentation for data utilities, **including device handling in DataLoader.**
    *   âŒ **Unit Tests:** Missing. Need tests for DataLoader covering collation, parallelism, device placement, `pin_memory`.
    *   âŒ **Parallel Loading Tests:** Missing.
    *   âŒ **Documentation:** Missing. Needs to detail device management in `DataLoader`, collation, `pin_memory`.
*   **Overall Status Phase 3:** **Not started.** All components related to this phase were removed. **DataLoader implementation requires careful consideration of device management, collation, and thread-safety.**

**Phase 4: GPU Acceleration (CUDA First, then Others) [â³ To Do]**
*   ğŸ¯ **Goal:** Enable high-performance computation using accelerators, starting with NVIDIA GPUs via CUDA, **leveraging the `Buffer`/`StorageDevice` abstraction and thread-safe `Tensor` structure.**
*   **4.1 Backend Abstraction Layer [â³]**
    *   â³ Define `StorageDevice` Enum/Struct more concretely if needed (e.g., `CPU`, `Cuda(gpu_id: u32)`). (Already `CPU`/`GPU`, needs refinement for multi-GPU).
    *   â³ Solidify `TensorData` structure containing `device: StorageDevice` and `data: Arc<Buffer<T>>` where `Buffer<T>` can be `Cpu(Arc<Vec<T>>)` or `Gpu(...)`.
    *   â³ Implement `Tensor::device()` method (âœ… Already Done, may need refinement for specific GPU IDs).
    *   â³ Implement `Tensor::to(device: StorageDevice)` method: Creates a *new* `Tensor` by copying data to a new `Buffer` allocated on the target device (CPU <-> GPU, GPU <-> GPU). Handles `Arc<RwLock>` correctly.
    *   â³ Design lazy initialization for CUDA contexts/devices.
*   **4.2 CUDA Integration & Infrastructure [â³]**
    *   â³ Select and integrate CUDA binding crate (e.g., `cuda-rs`, `cudarc`, `accel`).
    *   â³ Manage CUDA Contexts (creation, destruction, current context per thread).
    *   â³ Manage CUDA Streams (creation, synchronization - `cudaStreamSynchronize`, `cudaEventRecord`/`cudaStreamWaitEvent`) for asynchronous operations.
*   **4.3 GPU Memory Management [â³]**
    *   â³ Implement GPU memory allocation/deallocation (`cudaMalloc`, `cudaFree`) within the **`Buffer::Gpu`** variant.
    *   â³ Implement asynchronous data transfers (Host <-> Device, Device <-> Device) using streams (`cudaMemcpyAsync`) as part of `Tensor::to()` and potentially other operations.
    *   â³ Implement Pinned Memory allocation (`cudaMallocHost`/`cudaHostRegister`) to back `Buffer::Cpu` when `pin_memory=true` (used by DataLoader Phase 3).
    *   â³ Explore GPU memory pooling/caching allocators for `Buffer::Gpu` to reduce overhead.
*   **4.4 CUDA Kernels / Library Integration [â³]**
    *   â³ **Element-wise Ops:** Implement kernels/bindings operating on pointers extracted from `Buffer::Gpu` inputs, writing to a new `Buffer::Gpu` output.
    *   â³ **Reductions:** Implement kernels/bindings for reductions on `Buffer::Gpu` data.
    *   â³ **Matrix Multiplication:** Integrate cuBLAS (`cublas<t>gemm`), taking GPU buffer pointers as input.
    *   â³ **Convolutions:** Integrate cuDNN (`cudnnConvolution*`), configuring it with descriptors based on tensor metadata and GPU buffer pointers.
    *   â³ **Pooling:** Integrate cuDNN (`cudnnPooling*`).
    *   â³ **Activations:** Implement kernels or use cuDNN (`cudnnActivation*`).
    *   â³ **Indexing/Shape Ops:** Implement GPU kernels for `gather`, `scatter`, `slice`, `cat`, `stack` operating on `Buffer::Gpu`.
    *   â³ **Random Number Generation:** Integrate cuRAND (`curandGenerate*`) to create `Tensor`s with `Buffer::Gpu` directly.
*   **4.5 Framework Integration [â³]**
    *   â³ **Ops Dispatch:** Modify all op implementations (`neurarust-core::ops`, e.g., `add_op`) to: check `tensor.device()`; if all inputs are `StorageDevice::GPU`, call the corresponding CUDA kernel/library; if `CPU`, call CPU logic; otherwise error or copy.
    *   â³ **Autograd:** Ensure `BackwardOp` implementations have GPU variants. `backward()` calls must dispatch correctly based on device. Gradient accumulation must happen on the correct device (`Buffer::Gpu` or `Buffer::Cpu`).
    *   â³ **NN Layers:** Modify layers (`neurarust-core::nn`) to:
        *   Accept `device` on construction for `Parameter` initialization (creating `Buffer::Gpu` or `Buffer::Cpu`).
        *   Implement `.to(device)` using `Tensor::to()` for parameters/buffers.
        *   Rely on Ops Dispatch within their `forward` methods.
    *   â³ **DataLoader:** Integrate `pin_memory` option using GPU backend's pinned memory allocation.
*   **4.6 Mixed-Precision Training (AMP) [â³]**
    *   â³ Add `f16` / `bf16` support, likely primarily within `Buffer::Gpu`.
    *   â³ Implement `autocast` interacting with Ops Dispatch.
    *   â³ Implement `GradScaler` operating potentially on GPU loss `Tensor`.
    *   â³ Consider FP32 master weights pattern within optimizers.
*   **4.7 Multi-GPU Support (Single Node) [â³]**
    *   â³ Refine `StorageDevice::Gpu(id)` for device selection.
    *   â³ Implement basic `DataParallel` utility using `Tensor::to(device)` for placement and GPU communication libraries (e.g., NCCL bindings) operating on `Buffer::Gpu` pointers.
*   **4.8 Other Backends (Exploratory/Future) [â³]**
    *   â³ **ROCm (AMD):** Investigate HIP bindings, potential `Buffer::Hip` variant.
    *   â³ **Metal (Apple Silicon):** Investigate Metal bindings, potential `Buffer::Mtl` variant.
    *   â³ **WebGPU:** Explore `wgpu` crate, requires WGSL kernels, potential `Buffer::Wgpu` variant.
*   **4.9 Testing & Benchmarking [â³]**
    *   â³ Unit tests for GPU memory (`Buffer::Gpu`), H2D/D2H/D2D copies (`Tensor::to`).
    *   â³ Unit tests for individual GPU kernels/library calls (comparing results with CPU ops via `.to(CPU)`).
    *   â³ Integration tests for Autograd and NN layers operating on GPU `Tensor`s.
    *   â³ Tests for Mixed-Precision and Multi-GPU.
    *   â³ Benchmarks comparing CPU vs GPU performance for ops and models.
*   **4.10 Build & CI [â³]**
    *   â³ Implement conditional compilation (`cfg` features) for CUDA.
    *   â³ Set up CI with CUDA toolkit and GPU runners.
*   **4.11 Documentation [â³]**
    *   â³ Document CUDA setup.
    *   â³ Document `StorageDevice`, `Buffer::Gpu` concepts, `Tensor::to()`, device handling in ops/layers/training.
    *   â³ Document AMP and Multi-GPU usage.

**Phase 5: Advanced Features, Ecosystem & Usability [â³ To Do]**
*   ğŸ¯ **Goal:** Implement more complex NN architectures, improve interoperability, and enhance the overall developer experience, **fully integrating device management and leveraging the core abstractions.**
*   **5.1 Advanced NN Architectures & Modules [â³]**
    *   ğŸ¯ Goal: Build advanced, reusable NN components aware of device placement.
    *   â³ **Transformer Components:** Implement device-aware `MultiheadAttention`, `TransformerEncoderLayer`, etc., using device-aware ops.
    *   â³ **Advanced RNN Features:** Implement device-aware bidirectionality, `PackedSequence` handling (needs careful buffer/device management).
    *   â³ **Normalization Variants:** Implement `SyncBatchNorm` (requires multi-GPU communication on device buffers - Phase 4).
    *   â³ **Other Potential Modules:** Ensure device compatibility for new activations, attention mechanisms etc.
*   **5.2 ONNX Export/Import [â³]**
    *   ğŸ¯ Goal: Allow model exchange, **handling device differences.**
    *   â³ **Exporter:** Access parameters/buffers via locks from their respective devices (copying to CPU for serialization likely needed). Map device-aware NeuraRust ops to ONNX.
    *   â³ **Importer:** Parse ONNX, map ops. Load weights, placing them onto the user-specified `device` (via `map_location`) by creating appropriate `Tensor`s/`Buffer`s.
    *   â³ **Testing & Coverage:** Test exported models vs ONNX Runtime, considering device. Document supported ops and device implications.
*   **5.3 Python Bindings (PyO3) (`neurarust-py`) [â³]**
    *   ğŸ¯ Goal: Enable seamless Python integration, **exposing device management APIs.**
    *   â³ **Crate Setup:** Create `neurarust-py` with `PyO3`.
    *   â³ **Tensor Bindings:** Expose `Tensor` including `.device`, `.to(device)`. Handle NumPy conversion carefully regarding devices (copying, errors?). Expose `StorageDevice` enum.
    *   â³ **Autograd Bindings:** Ensure compatibility with device-aware tensors.
    *   â³ **NN Module Bindings:** Expose device-aware `nn.Module` (with `.to()`), layers, losses.
    *   â³ **Optimizer & Scheduler Bindings:** Expose device-aware versions.
    *   â³ **DataLoader Bindings:** Expose device options (`pin_memory`, target device).
    *   â³ **Packaging & Distribution:** Configure `maturin`.
    *   â³ **Testing:** Python-side tests covering device interactions.
    *   â³ **Documentation:** Provide Python API docs with device handling examples.
*   **5.4 JIT Compilation / Graph Optimization (Exploratory) [â³]**
    *   ğŸ¯ Goal: Explore static graph optimization, **considering device-specific opportunities.**
    *   â³ **Tracing/Scripting:** Capture device information.
    *   â³ **Intermediate Representation (IR):** Must encode device placement.
    *   â³ **Optimization Passes:** Operator Fusion, etc., must be device-aware.
    *   â³ **Code Generation:** Generate code dispatching to correct device backends (CPU/GPU).
*   **5.5 Visualization & Debugging [â³]**
    *   ğŸ¯ Goal: Improve developer experience for understanding device-aware models.
    *   â³ **Training Hooks:** Access data via Buffers/locks (copy to CPU if needed).
    *   â³ **Computation Graph Visualization:** Indicate device placement.
    *   â³ **Debugging Tools:** Numerical gradient checking adapted (Phase 1). Improve device mismatch errors.
*   **5.6 Documentation, Examples & Tutorials [â³]**
    *   ğŸ¯ Goal: Provide comprehensive resources covering device management thoroughly.
    *   â³ **Comprehensive User Guide:** Cover `StorageDevice`, `.to()`, device handling in all components, CPU vs GPU training loops.
    *   â³ **API Reference Documentation:** Ensure `rustdoc` clearly explains device parameters/returns/implications.
    *   â³ **Gallery of Examples:** Provide examples running on CPU and GPU.
    *   â³ **Tutorials:** Cover device management explicitly.
    *   â³ **Project Website:** Host all device-aware documentation.

**Phase 6: Deployment, Specialization & Maturity [â³ To Do]**
*   ğŸ¯ **Goal:** Target specific deployment platforms, leverage Rust's unique strengths, implement distributed training, and foster a community, **all built upon the device-aware and thread-safe core.**
*   **6.1 Deployment Targets [â³]**
    *   ğŸ¯ Goal: Enable efficient deployment across diverse environments using appropriate backends.
    *   â³ **WebAssembly (WASM):** Compile core targeting CPU backend (`Buffer::Cpu`). Exclude GPU code via `cfg`. Consider single/multi-thread implications for `Arc<RwLock>`. Explore WebGPU backend later (Phase 4.8).
    *   â³ **Native Binary Deployment:** Leverage optimized CPU backend, or GPU backend if built with CUDA `cfg`. Facilitate static linking.
    *   â³ **Edge/Embedded (ARM):** Target performant CPU backend (`Buffer::Cpu`), consider `Arc<RwLock>` overhead and NEON potential.
    *   â³ **Server-Side Inference:** Utilize CPU/GPU backend. Leverage thread-safe `Tensor`s for sharing models across request threads.
*   **6.2 Inference Optimizations [â³]**
    *   ğŸ¯ Goal: Reduce model size and accelerate inference speed using device-aware techniques.
    *   â³ **Quantization:** Implement device-aware quantization (PTQ, QAT). Requires quantized kernels/ops for CPU/GPU. May need `Buffer` variants or metadata for quantized types.
    *   â³ **Pruning:** Implement device-aware pruning application. Explore sparse `Buffer` representations/kernels.
    *   â³ **Model Distillation:** Support depends on running device-aware models and loss calculations.
*   **6.3 Distributed Training (Multi-Node) [â³]**
    *   ğŸ¯ Goal: Enable large-scale training using device-specific communication backends.
    *   â³ **Communication Backend Integration:** Integrate MPI/Gloo (CPU `Buffer`) or NCCL (GPU `Buffer`).
    *   â³ **Distributed Primitives:** Implement collectives operating on specific `Buffer` types via appropriate backends.
    *   â³ **`DistributedDataParallel` (DDP):** Implement using device placement (`Tensor::to`), device-aware autograd, and communication primitives on device buffers.
*   **6.4 Leveraging Rust's Strengths [â³]**
    *   ğŸ¯ Goal: Fully exploit Rust's features enabled by the core architecture.
    *   â³ **Advanced Static Optimizations (Compile-Time):** Explore device-aware macro-based optimizations.
    *   â³ **Enhanced Safety & Verification:** Leverage thread-safety of `Arc<RwLock>`. Verify `unsafe` blocks in `Buffer` management / FFI.
    *   â³ **Fearless Concurrency:** Utilize `rayon` for CPU ops thanks to thread-safe `Tensor`. Explore task-based parallelism.
*   **6.5 Tooling & Infrastructure [â³]**
    *   ğŸ¯ Goal: Provide robust development tools reflecting the multi-device nature.
    *   â³ **Robust Benchmarking Suite:** Benchmark ops/models across CPU/GPU, comparing `Buffer` backends.
    *   â³ **Extended Continuous Integration (CI):** Test builds/runs across platforms, devices (CPU/GPU runners), and feature flags (`cfg`).
*   **6.6 Community & Ecosystem [â³]**
    *   ğŸ¯ Goal: Foster an active community knowledgeable about the device-aware architecture.
    *   â³ **Governance & Contribution:** Establish clear processes.
    *   â³ **Community Engagement:** Communicate clearly about device support/usage.
    *   â³ **Ecosystem Integration:** Ensure integrations correctly handle device-aware `Tensor`s.

*(This highly detailed roadmap reflects the long-term ambition. Priorities and specific implementation details will evolve based on progress, community feedback, and emerging needs.)*

---

## ğŸ“‚ Project Structure

Here is an overview of the current NeuraRust project layout:

```
.
â”œâ”€â”€ Cargo.lock                # Generated by Cargo, locks dependency versions.
â”œâ”€â”€ Cargo.toml                # Main workspace manifest (defines members, dependencies, metadata).
â”œâ”€â”€ CODE_OF_CONDUCT.md      # Code of conduct for contributors.
â”œâ”€â”€ CONTRIBUTING.md         # Guide for contributing to the project.
â”œâ”€â”€ Goals.md                  # This file: Project vision, goals, and roadmap.
â”œâ”€â”€ LICENSE                   # Project license (e.g., MIT, Apache 2.0).
â”œâ”€â”€ neurarust-core            # Core crate containing the heart of the framework.
â”‚Â Â  â”œâ”€â”€ Cargo.toml            # Manifest for the neurarust-core crate.
â”‚Â Â  â”œâ”€â”€ src                   # Source code for neurarust-core.
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ autograd          # Module for automatic differentiation.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ graph.rs      # Functions for graph traversal (e.g., topological sort).
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ mod.rs        # Defines the autograd engine traits (`BackwardOp`) and logic.
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ lib.rs            # Entry point for the neurarust-core library (re-exports modules).
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ nn                # Module for neural network components.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ layers        # Submodule for different network layers.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ linear.rs # Implementation of the Linear (fully connected) layer.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ mod.rs    # Declares the layers submodule and potentially re-exports layers.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ losses        # Submodule for loss functions.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mod.rs    # Declares the losses submodule and re-exports loss functions.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ mse.rs    # Implementation of Mean Squared Error (MSE) loss.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mod.rs        # Declares the nn module and re-exports key components (`Module`, `Parameter`, layers, losses).
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ module.rs     # Defines the base `Module` trait for all nn components.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ parameter.rs  # Defines the `Parameter` struct for trainable model weights.
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ops               # Module containing tensor operations.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ activation    # Submodule for activation functions.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mod.rs    # Declares the activation submodule.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ relu.rs   # Implementation of the ReLU activation function.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ arithmetic    # Submodule for element-wise arithmetic operations.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ add.rs    # Implementation of addition.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ div.rs    # Implementation of division.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mod.rs    # Declares the arithmetic submodule.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mul.rs    # Implementation of multiplication.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ neg.rs    # Implementation of negation.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ pow.rs    # Implementation of exponentiation.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ sub.rs    # Implementation of subtraction.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ indexing.rs   # Tensor indexing and slicing operations (`slice_op`).
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ linalg        # Submodule for linear algebra operations.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ matmul.rs # Implementation of matrix multiplication.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mod.rs    # Declares the linalg submodule.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ transpose.rs# Implementation of tensor transposition.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ loss          # Submodule for loss-related operations (currently empty/declarative).
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ mod.rs    # Declares the loss operations submodule.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ math_elem     # Submodule for element-wise mathematical functions.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mod.rs    # Declares the math_elem submodule.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ sqrt.rs   # Implementation of square root.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mod.rs        # Declares ops submodules and re-exports common operations.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ reduction     # Submodule for reduction operations (sum, mean, max...). 
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mod.rs    # Declares the reduction submodule.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ sum.rs    # Implementation of sum reduction.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ stack.rs      # Operation for stacking tensors along a new dimension.
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tensor            # Module defining the Tensor struct and its methods.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mod.rs        # Defines the `Tensor` struct, its core methods (creation, shape, data access), and autograd integration (`backward`, `grad`).
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ utils.rs      # Tensor utility functions (e.g., broadcasting helpers like `broadcast_shapes`, `reduce_gradient`).
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ tensor_data.rs  # Defines the internal `TensorData` struct holding data, shape, grad status, etc.
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ utils             # Module for general utility functions within the crate.
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ testing.rs    # Utility functions specifically for testing purposes.
â”‚Â Â  â”‚Â Â  â””â”€â”€ utils.rs        # Top-level utils file in src (potentially for re-exports or crate-wide utils - currently seems minimal).
â”‚Â Â  â””â”€â”€ tests                 # Directory containing integration tests for neurarust-core.
â”‚Â Â      â””â”€â”€ training_loop.rs# Integration test simulating a basic training loop (likely incomplete).
â”œâ”€â”€ neurarust-data            # Crate for data loading and handling utilities.
â”‚Â Â  â”œâ”€â”€ Cargo.toml            # Manifest for the neurarust-data crate.
â”‚Â Â  â””â”€â”€ src                   # Source code for neurarust-data.
â”‚Â Â      â”œâ”€â”€ dataloader.rs   # Implementation of the `DataLoader` for batching and iterating over datasets.
â”‚Â Â      â”œâ”€â”€ lib.rs            # Entry point for neurarust-data library (defines `Dataset` trait, re-exports components).
â”‚Â Â      â””â”€â”€ vec_dataset.rs    # Simple `Dataset` implementation backed by a `Vec`.
â”œâ”€â”€ neurarust-optim           # Crate dedicated to optimization algorithms.
â”‚Â Â  â”œâ”€â”€ Cargo.toml            # Manifest for the neurarust-optim crate.
â”‚Â Â  â””â”€â”€ src                   # Source code for neurarust-optim.
â”‚Â Â      â”œâ”€â”€ adam.rs         # Implementation of the Adam optimizer.
â”‚Â Â      â”œâ”€â”€ lib.rs            # Entry point for neurarust-optim library (defines `Optimizer` trait, re-exports optimizers).
â”‚Â Â      â””â”€â”€ sgd.rs          # Implementation of the SGD (Stochastic Gradient Descent) optimizer.
â””â”€â”€ README.md                 # Main project README file.
```

---

*(Note: Descriptions are based on file names, module structure, and content analysis. They can be further refined as the project evolves.)*