# ‚ú® NeuraRust Goals & Vision ü¶Äüß†

**NeuraRust** aims to become a leading **Deep Learning framework in Rust**, combining the flexibility and ergonomics of PyTorch with the **raw performance**, **memory safety**, and **portability** offered by Rust.

---

## üéØ Our Core Pillars

*   üöÄ **Exceptional Performance:**
    *   Compete with C++/Python giants in execution speed (CPU & GPU).
    *   Minimize memory footprint thanks to Rust's precise control (no GC!).
*   ü§ù **Intuitive Ergonomics:**
    *   A familiar and enjoyable API, inspired by best practices (PyTorch/Keras).
    *   Comprehensive documentation and accessible tutorials for a quick start.
*   üîÑ **Seamless Interoperability:**
    *   Compatibility via **ONNX** to exchange models with PyTorch/TensorFlow.
    *   Smooth integration with the Python ecosystem using **PyO3**.
*   üîí **Safety & Easy Deployment:**
    *   The Rust promise: **No segfaults, no unexpected memory leaks**.
    *   Native support for easy deployment to various targets: **WebAssembly (WASM)**, **ARM** (embedded/mobile), servers...

---

## üõ†Ô∏è Core Features (PyTorch Inspired, Rust Superpowered)

We replicate PyTorch's essential building blocks, enhancing them with Rust:

### 1. Multi-Dimensional Tensors (`neurarust-core::Tensor`) üìê

*   **Vision:** The framework's beating heart. Fast, safe, flexible.
*   **Key Points:**
    *   **Explicit and performant** memory management.
    *   Fine-grained control over memory layout (strides...).
    *   **Strong typing** to catch dimension/type errors at compile time (via `DType` enum now).
    *   Mathematical, logical operations, indexing, **broadcasting**... everything needed!
*   **The Rust Advantage:** üí™ Guaranteed memory safety, native C/C++ performance, SIMD potential.

### 2. Automatic Differentiation (`neurarust-core::Autograd`) üìà

*   **Vision:** A dynamic, reliable, and efficient autograd engine.
*   **Key Points:**
    *   **On-the-fly computation graph** construction.
    *   Simplified gradient calculation via **`.backward()`**.
    *   Optimized memory management for intermediate tensors.
    *   Backward pass implemented for core ops (validated manually).
*   **The Rust Advantage:** üß† The borrow checker tames graph complexity, "fearless" concurrency potential for accelerating computations.

### 3. Neural Network Modules (`neurarust-nn`) üß© *(Future)*

*   **Vision:** A comprehensive toolbox for assembling your networks.
*   **Key Points:**
    *   Standard layers: **Linear, Convolutional, Recurrent, Attention, Normalization...**
    *   Common activation and loss functions.
    *   **Composable and extensible** API for creating custom architectures.
*   **The Rust Advantage:** ‚ú® Traits for clear interfaces (`Module`, `Layer`), macros for less boilerplate.

### 4. Optimizers (`neurarust-optim`) ‚öôÔ∏è *(Future)*

*   **Vision:** Essential algorithms for training your models.
*   **Key Points:**
    *   Classics: **SGD**, Adam, AdamW, RMSprop...
    *   Simple `Optimizer` interface to apply updates.
    *   Internal state management (e.g., moments).
*   **The Rust Advantage:** ‚ö° Native performance, generic implementations via traits.

### 5. Data Loading (`neurarust-data`) üíæ *(Future)*

*   **Vision:** Performant tools to feed your models.
*   **Key Points:**
    *   `Dataset` and `DataLoader` abstractions.
    *   **Batching, shuffling, performant parallel loading**.
    *   Utilities for transformations and augmentations.
*   **The Rust Advantage:** üèéÔ∏è Robust parallelism ideal for I/O and preprocessing, efficient memory management.

### 6. Accelerator Support (GPU & Beyond) üî• *(Future)*

*   **Vision:** Unleash the massive computational power of dedicated hardware.
*   **Key Points:**
    *   **CUDA** integration (priority), then potentially ROCm, Metal, **WebGPU**.
    *   `Device` abstraction (CPU, GPU:0...).
    *   Transparent CPU <-> GPU data transfer.
*   **The Rust Advantage:** üåê Existing bindings, safe abstractions, WebGPU (written in Rust) as a portable target for the future.

### 7. Interoperability & Deployment (`neurarust-deploy`) üåç *(Future)*

*   **Vision:** Integrate everywhere, deploy easily.
*   **Key Points:**
    *   **ONNX** for model exchange.
    *   **PyO3** for symbiosis with Python.
    *   **WASM** compilation for web and serverless.
    *   Easy cross-compilation (e.g., **ARM**).
    *   **Native, standalone, and performant** binaries.
*   **The Rust Advantage:** üì¶ First-class WASM/ARM support, mature FFI, easy static binary distribution.

---

## üíé Our Differentiators: The Unique Rust Advantage

Beyond PyTorch parity, we aim to fully leverage Rust to offer:

*   **First-Class WASM Support üï∏Ô∏è:** Performant and lightweight inference in the browser and on the edge. Revolutionizing interactive and embedded ML.
*   **Enhanced Safety Guarantees ‚úÖ:** Go further in verification and robustness using the type system for critical applications.
*   **Advanced Static Optimizations üöÄ:** Use macros to optimize graphs *at compile time* (op fusion, etc.) for more performance with no runtime overhead.
*   **Simplified & Safe Parallelism ‚õìÔ∏è:** High-level APIs to leverage multi-core and distributed computing without fearing data races.

---

## üó∫Ô∏è Highly Detailed Roadmap (PyTorch Parity Goal)

This roadmap outlines the planned development stages for NeuraRust, aiming for extensive feature parity with PyTorch over time. Status markers: ‚úÖ (Done), üöß (In Progress / Partially Done), ‚è≥ (To Do), ‚ùå (Needs Rework / Blocked).

### Milestone 1: Core Ops & Basic Autograd (End Goal: Trainable MLP)

*   [X] **1.1 Project Setup:** Initial structure, `Cargo.toml`, basic Tensor struct (data, shape). **DONE**
*   [X] **1.2 Basic Tensor Creation:** `new`, `zeros`, `ones`, `rand`, `randn`, `full`, `from_vec`, `eye`. **DONE**
*   [X] **1.3 Basic Tensor Ops:** `reshape`, `transpose`, `permute`, `contiguous`, `slice`, element access (initial impl). **DONE** (Element access basic impl via `get_f32_data` exists, but dedicated `get`/`at` method pending -> tests ignored).
*   [~] **1.4 Gradient Checking Utility (`check_grad`):** Implement numerical gradient checking. **PARTIALLY DONE**
    *   Initial implementation done.
    *   Refactored perturbation logic to handle views correctly.
    *   Identified F32/F64 precision limitations with finite differences, especially for view operations (`permute`, `transpose`) and potentially `matmul`.
    *   **Action:** Relevant backward tests (`permute`, `transpose >2D`, `matmul` non-simple) are marked `#[ignore]` until `check_grad` is improved or replaced.
*   [X] **1.5 Autograd Infrastructure:** `TensorData` with `requires_grad`, `grad`, `grad_fn`. Computation graph (`BackwardOp` trait, node tracking). **DONE**
*   [X] **1.6 Basic Arithmetic Ops (+ Autograd):** `add`, `sub`, `mul`, `div`, `neg`, `pow`. Implement forward and backward passes. **DONE**
*   [X] **1.7 View Ops Autograd:** Implement backward passes for `reshape`, `transpose`, `permute`, `slice`, `contiguous`. **DONE** (`permute` backward logic implemented, but tests ignored due to 1.4).
*   [X] **1.8 Reduction Ops (+ Autograd):** `sum`, `mean`, `max`. Implement forward and backward passes. **DONE**
*   [X] **1.9 Basic Linear Algebra (+ Autograd):** `matmul`. Implement forward and backward passes. **DONE** (Backward tests beyond simple case ignored due to 1.4).
*   [X] **1.10 Activation Functions (+ Autograd):** `relu`, `sigmoid` (optional), `tanh` (optional), `softmax` (optional). **DONE** (ReLU implemented).
*   [ ] **1.11 Documentation & Cleanup:** Docstrings for public APIs, README update, code cleanup (`cargo fmt`, `cargo clippy`, remove warnings/dead code). **IN PROGRESS**
*   [ ] **1.12 MLP Layer:** Implement a basic linear layer (`nn.Linear`).
*   [ ] **1.13 Loss Function:** Implement Mean Squared Error (`nn.MSELoss`).
*   [ ] **1.14 Basic Training Loop:** Put it all together to train a simple MLP on dummy data.

### Milestone 2: Optimizers & Advanced Features

*   [ ] **2.1 Optimizers:** SGD, Adam.
*   [ ] **2.2 More Ops:** Convolutions (2D), Pooling.
*   [ ] **2.3 GPU Support (CUDA/WGPU):** Abstract `StorageDevice` and `Buffer`, implement GPU kernels.
*   [ ] **2.4 Serialization:** Saving and loading models.
*   [ ] **2.5 Advanced Indexing/Slicing:** More Pythonic indexing (e.g., `tensor[:, 0, ::2]`).
*   [ ] **2.6 Data Loading Utilities.**

### Milestone 3: Ecosystem & Polish

*   [ ] **3.1 Benchmarking.**
*   [ ] **3.2 Integration Tests.**
*   [ ] **3.3 Extended Examples.**
*   [ ] **3.4 Contributions Guide.**

---
